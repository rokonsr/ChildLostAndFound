{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKx9Ay3m2Odq"
      },
      "source": [
        "# Downloading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZjM8n5j0kZB",
        "outputId": "3bbc6d4a-ce19-4df4-dfcd-1a585358533b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wOPFp8-_k8c",
        "outputId": "6956964c-3783-4310-af1a-54528f39f985"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-21 11:11:24--  https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/wiki_crop.tar\n",
            "Resolving data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)... 129.132.52.178, 2001:67c:10ec:36c2::178\n",
            "Connecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|129.132.52.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 811315200 (774M) [application/x-tar]\n",
            "Saving to: ‘wiki_crop.tar’\n",
            "\n",
            "wiki_crop.tar       100%[===================>] 773.73M  22.4MB/s    in 38s     \n",
            "\n",
            "2024-12-21 11:12:03 (20.6 MB/s) - ‘wiki_crop.tar’ saved [811315200/811315200]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/wiki_crop.tar\n",
        "!tar -xf wiki_crop.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeU_6DdsuvKa"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "\n",
        "import pickle as pkl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms, utils\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXdydrBGGDRH"
      },
      "source": [
        "# Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NEp6vcEZjhS"
      },
      "outputs": [],
      "source": [
        "def load_data(dataset='wiki', data_dir='./wiki_crop'):\n",
        "\n",
        "    meta_path = Path(data_dir) / f'{dataset}.mat'\n",
        "    meta = loadmat(meta_path)\n",
        "    meta_data = meta[dataset][0, 0]\n",
        "\n",
        "    full_path = meta_data['full_path'][0]\n",
        "    full_path = [y for x in full_path for y in x]\n",
        "\n",
        "    dob = meta_data['dob'][0]\n",
        "\n",
        "    photo_taken = meta_data['photo_taken'][0]\n",
        "\n",
        "    age = [calc_age(photo_taken[i], dob[i]) for i in range(len(dob))]\n",
        "\n",
        "    clean_mapping = {pth:age for (pth, age) in zip(full_path, age) if age > 0}\n",
        "\n",
        "    full_path = list(clean_mapping.keys())\n",
        "    age = list(clean_mapping.values())\n",
        "\n",
        "    return full_path, age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYEcMKo1TpUh"
      },
      "outputs": [],
      "source": [
        "def calc_age(taken, dob):\n",
        "\n",
        "    birth = datetime.fromordinal(max(int(dob) - 366, 1))\n",
        "\n",
        "    if birth.month < 7:\n",
        "        return taken - birth.year\n",
        "    else:\n",
        "        return taken - birth.year - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZ4JPgJTSr7h"
      },
      "outputs": [],
      "source": [
        "def scale(x, feature_range=(-1, 1)):\n",
        "\n",
        "    min, max = feature_range\n",
        "    x = x * (max - min) + min\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCOceOxSZ4Vr"
      },
      "outputs": [],
      "source": [
        "bins = [18, 29, 39, 49, 59]\n",
        "def one_hot(x, bins):\n",
        "\n",
        "    x = x.numpy()\n",
        "    idxs = np.digitize(x, bins, right=True)\n",
        "    idxs = idxs.reshape(-1,1)\n",
        "    z = torch.zeros(len(x), len(bins)+1).scatter_(1, torch.tensor(idxs), 1)\n",
        "    return z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA3GehgpZ8U1"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_l6WZxaZ5h3"
      },
      "outputs": [],
      "source": [
        "class ImageAgeDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, data_dir, transform=None):\n",
        "\n",
        "        self.data_dir = data_dir\n",
        "        self.full_path, self.age = load_data(dataset, data_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.age)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(os.path.join(self.data_dir, self.full_path[idx]))\n",
        "        age = self.age[idx]\n",
        "        sample = {'image': image, 'age': age}\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fcCFgFSaByO"
      },
      "outputs": [],
      "source": [
        "class Resize(object):\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, age = sample['image'], sample['age']\n",
        "        image = transforms.Resize(self.output_size)(image)\n",
        "        return {'image': image, 'age': age}\n",
        "\n",
        "class ToTensor(object):\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, age = sample['image'], sample['age']\n",
        "        image = transforms.ToTensor()(image)\n",
        "        if image.size()[0] == 1:\n",
        "            image = image.expand(3,-1,-1)\n",
        "        return {'image': image, 'age': age}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxZr94rCDj1g"
      },
      "outputs": [],
      "source": [
        "dataset='wiki'\n",
        "data_dir='./wiki_crop'\n",
        "bins = [18, 29, 39, 49, 59]\n",
        "img_size = 64\n",
        "batch_size = 128\n",
        "\n",
        "tfms = transforms.Compose([Resize((img_size, img_size)),\n",
        "                           ToTensor()])\n",
        "\n",
        "train_dataset = ImageAgeDataset(dataset, data_dir, transform=tfms)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYLNycoYd8ON"
      },
      "outputs": [],
      "source": [
        "trainiter = iter(train_loader)\n",
        "train = next(trainiter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYarta8I0kZF"
      },
      "outputs": [],
      "source": [
        "# Get the image from the dataset\n",
        "image1 = train_dataset[0]['image'].numpy()\n",
        "\n",
        "# Convert the numpy array to PIL image\n",
        "image_pil = Image.fromarray(np.uint8(image1.transpose((1, 2, 0)) * 255))\n",
        "\n",
        "# Display the image\n",
        "image_pil.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDVrksNs0kZG"
      },
      "outputs": [],
      "source": [
        "# Get the image from the dataset\n",
        "image2 = train_dataset[60941]['image'].numpy()\n",
        "\n",
        "# Convert the numpy array to PIL image\n",
        "image_pil = Image.fromarray(np.uint8(image2.transpose((1, 2, 0)) * 255))\n",
        "\n",
        "# Display the image\n",
        "image_pil.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pblP2iGPeCnq"
      },
      "outputs": [],
      "source": [
        "#plt.imshow(train_dataset[0]['image'].numpy().transpose(1,2,0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nZCoiYYaxVG"
      },
      "outputs": [],
      "source": [
        "#plt.imshow(train_dataset[60941]['image'].numpy().transpose(1,2,0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o0Xtmhf0kZG"
      },
      "outputs": [],
      "source": [
        "from PIL import Image , ImageDraw\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "data = next(dataiter)\n",
        "images, labels = data['image'], data['age']\n",
        "\n",
        "fig = Image.new('RGB', (2500, 400))\n",
        "\n",
        "plot_size=20\n",
        "width = 125\n",
        "height = 200\n",
        "\n",
        "for idx in np.arange(plot_size):\n",
        "    img = np.transpose(images[idx], (1, 2, 0))\n",
        "    pil_img = Image.fromarray(np.uint8(img*255))\n",
        "    pil_img = pil_img.resize((width, height), Image.Resampling.LANCZOS)\n",
        "    fig.paste(pil_img, (idx*width, 0))\n",
        "    draw = ImageDraw.Draw(fig)\n",
        "    draw.text((idx*width+10, height+10), str(labels[idx].item()), fill='black')\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dE2zB4_Vc5wW"
      },
      "outputs": [],
      "source": [
        "# dataiter = iter(train_loader)\n",
        "# data = next(dataiter)\n",
        "# images, labels = data['image'], data['age']\n",
        "\n",
        "# fig = plt.figure(figsize=(25, 4))\n",
        "# plot_size=20\n",
        "# for idx in np.arange(plot_size):\n",
        "#     ax = fig.add_subplot(2, plot_size//2, idx+1, xticks=[], yticks=[])\n",
        "#     ax.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
        "#     ax.set_title(str(labels[idx].item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtz8TiR0HW8a"
      },
      "source": [
        "# Define Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tjfSolfHf9n"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGR-ofHuEFf4"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8vdR9BDFP0C"
      },
      "outputs": [],
      "source": [
        "def conv(in_channels, out_channels, kernel_size=4, stride=2, padding=1, batch_norm=True):\n",
        "    layers = []\n",
        "    conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
        "    layers.append(conv_layer)\n",
        "\n",
        "    if batch_norm:\n",
        "        bn = nn.BatchNorm2d(out_channels)\n",
        "        layers.append(bn)\n",
        "\n",
        "    return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Gl3qi7eIgxG"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, y_size, conv_dim=64):\n",
        "\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv_dim = conv_dim\n",
        "        self.y_size = y_size\n",
        "        self.conv1 = conv(3, conv_dim, 4, batch_norm=False)\n",
        "        self.conv2 = conv(conv_dim+y_size, conv_dim * 2, 4)\n",
        "        self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\n",
        "        self.conv4 = conv(conv_dim*4, conv_dim*8, 4)\n",
        "        self.conv5 = conv(conv_dim*8, 1, 4, 1, 0, batch_norm=False)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        y = y.view(-1,y.size()[-1],1,1)\n",
        "        y = y.expand(-1,-1,x.size()[-2], x.size()[-1])\n",
        "        x = torch.cat([x, y], 1)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.conv5(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0Q3nuJOHz1o"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXL4DQo5IvJm"
      },
      "outputs": [],
      "source": [
        "def deconv(in_channels, out_channels, kernel_size=4, stride=2, padding=1, batch_norm=True):\n",
        "\n",
        "    layers = []\n",
        "    t_conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
        "    layers.append(t_conv)\n",
        "\n",
        "    if batch_norm:\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "\n",
        "    return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg_8hGGg9KJL"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, z_size, y_size, conv_dim=64):\n",
        "\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.conv_dim = conv_dim\n",
        "\n",
        "        self.t_conv1 = deconv(z_size+y_size, conv_dim*8, 4, 1, 0)\n",
        "        self.t_conv2 = deconv(conv_dim*8, conv_dim*4, 4)\n",
        "        self.t_conv3 = deconv(conv_dim*4, conv_dim*2, 4)\n",
        "        self.t_conv4 = deconv(conv_dim*2, conv_dim, 4)\n",
        "        self.t_conv5 = deconv(conv_dim, 3, 4, batch_norm=False)\n",
        "\n",
        "    def forward(self, z, y):\n",
        "\n",
        "        x = torch.cat([z, y], dim=1)\n",
        "        x = x.view(-1, x.size()[-1], 1, 1)\n",
        "        x = F.relu(self.t_conv1(x))\n",
        "        x = F.relu(self.t_conv2(x))\n",
        "        x = F.relu(self.t_conv3(x))\n",
        "        x = F.relu(self.t_conv4(x))\n",
        "        x = self.t_conv5(x)\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGsty3wYIFRq"
      },
      "source": [
        "## Build complete network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMZsaBy79lPw",
        "outputId": "dde1dff1-67d0-4f0c-f108-23c051e6393a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminator(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(70, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv5): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "  )\n",
            ")\n",
            "\n",
            "Generator(\n",
            "  (t_conv1): Sequential(\n",
            "    (0): ConvTranspose2d(106, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (t_conv2): Sequential(\n",
            "    (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (t_conv3): Sequential(\n",
            "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (t_conv4): Sequential(\n",
            "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (t_conv5): Sequential(\n",
            "    (0): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "conv_dim = 64\n",
        "z_size = 100\n",
        "y_size = 6\n",
        "\n",
        "D = Discriminator(y_size, conv_dim)\n",
        "G = Generator(z_size, y_size, conv_dim)\n",
        "\n",
        "print(D)\n",
        "print()\n",
        "print(G)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oVuUkypIO7S"
      },
      "source": [
        "## Discriminator and Generator Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScKFn1ZB9qbY",
        "outputId": "ab591335-a88c-4846-8792-125a63296380"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7_sEm4fISzB"
      },
      "outputs": [],
      "source": [
        "def real_loss(D_out, smooth=False):\n",
        "    batch_size = D_out.size(0)\n",
        "    if smooth:\n",
        "        labels = torch.ones(batch_size)*0.9\n",
        "    else:\n",
        "        labels = torch.ones(batch_size)\n",
        "    labels = labels.to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    loss = criterion(D_out.squeeze(), labels)\n",
        "    return loss\n",
        "\n",
        "def fake_loss(D_out):\n",
        "    batch_size = D_out.size(0)\n",
        "    labels = torch.zeros(batch_size)\n",
        "    labels = labels.to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    loss = criterion(D_out.squeeze(), labels)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hra3ltXxIYH7"
      },
      "source": [
        "## Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFZqaTTsIVfq"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "lr = 0.0002\n",
        "beta1=0.5\n",
        "beta2=0.999\n",
        "\n",
        "d_optimizer = optim.Adam(D.parameters(), lr, [beta1, beta2])\n",
        "g_optimizer = optim.Adam(G.parameters(), lr, [beta1, beta2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4As0AzGIfWS"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdRK5VL9VJFq"
      },
      "outputs": [],
      "source": [
        "def checkpoint(G, D, epoch, model, root_dir):\n",
        "    target_dir = f'{root_dir}/{model}'\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "    G_path = os.path.join(target_dir, f'G_{epoch}.pkl')\n",
        "    D_path = os.path.join(target_dir, f'D_{epoch}.pkl')\n",
        "    torch.save(G.state_dict(), G_path)\n",
        "    torch.save(D.state_dict(), D_path)\n",
        "\n",
        "def oh_to_class(fixed_y):\n",
        "    age_map = {0:'0-18',1:'19-29',2:'30-39',3:'40-49',4:'50-59',5:'60+'}\n",
        "    if torch.cuda.is_available():\n",
        "        fixed_y = fixed_y.cpu()\n",
        "    fixed_y_idxs = fixed_y.numpy().nonzero()[1]\n",
        "    fixed_y_ages = [age_map[idx] for idx in fixed_y_idxs]\n",
        "\n",
        "    return fixed_y_ages\n",
        "\n",
        "def save_samples_ages(samples, fixed_y, model, root_dir):\n",
        "    fixed_y_ages = oh_to_class(fixed_y)\n",
        "    samples_ages = {'samples': samples, 'ages': fixed_y_ages}\n",
        "    target_dir = f'{root_dir}/{model}'\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "    with open(f'{target_dir}/train_samples_ages.pkl', 'wb') as f:\n",
        "        pkl.dump(samples_ages, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vra3o0xdIb5m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o29wZijH0kZI",
        "outputId": "06e005ba-123d-4d01-d397-b4555ab1a02c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [    1/    5] | d_loss: 1.4621 | g_loss: 2.4505\n",
            "Epoch [    1/    5] | d_loss: 0.6301 | g_loss: 2.5715\n",
            "Epoch [    2/    5] | d_loss: 0.4964 | g_loss: 3.0716\n",
            "Epoch [    2/    5] | d_loss: 0.7029 | g_loss: 3.3397\n",
            "Epoch [    3/    5] | d_loss: 0.3784 | g_loss: 3.2694\n",
            "Epoch [    3/    5] | d_loss: 0.6673 | g_loss: 2.0259\n",
            "Epoch [    4/    5] | d_loss: 0.4380 | g_loss: 3.0751\n",
            "Epoch [    4/    5] | d_loss: 0.4516 | g_loss: 4.5311\n",
            "Epoch [    5/    5] | d_loss: 0.2761 | g_loss: 2.3957\n",
            "Epoch [    5/    5] | d_loss: 0.3317 | g_loss: 3.5414\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import pickle as pkl\n",
        "\n",
        "root_dir = '/content/Age-cGAN'\n",
        "model = 'GAN_1'\n",
        "os.makedirs(root_dir, exist_ok=True)\n",
        "\n",
        "G.to(device)\n",
        "D.to(device)\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "samples = []\n",
        "losses = []\n",
        "\n",
        "print_every = 300\n",
        "\n",
        "sample_size=16\n",
        "fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n",
        "fixed_z = torch.from_numpy(fixed_z).float()\n",
        "fixed_y = np.random.randint(len(bins), size=sample_size)\n",
        "fixed_y = fixed_y.reshape(-1,1)\n",
        "fixed_y = torch.zeros(sample_size, len(bins)+1).scatter_(1, torch.tensor(fixed_y, dtype=torch.long), 1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    for batch_i, batch in enumerate(train_loader):\n",
        "\n",
        "        batch_size = batch['image'].size(0)\n",
        "\n",
        "        real_images = scale(batch['image'])\n",
        "\n",
        "        ages = one_hot(batch['age'], bins)\n",
        "\n",
        "        d_optimizer.zero_grad()\n",
        "\n",
        "        real_images = real_images.to(device)\n",
        "        ages = ages.to(device)\n",
        "\n",
        "        D_real = D(real_images, ages)\n",
        "        d_real_loss = real_loss(D_real)\n",
        "\n",
        "        z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
        "        z = torch.from_numpy(z).float()\n",
        "        z = z.to(device)\n",
        "        fake_images = G(z, ages)\n",
        "\n",
        "        D_fake = D(fake_images, ages)\n",
        "        d_fake_loss = fake_loss(D_fake)\n",
        "\n",
        "        d_loss = d_real_loss + d_fake_loss\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        g_optimizer.zero_grad()\n",
        "\n",
        "        z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
        "        z = torch.from_numpy(z).float()\n",
        "        z = z.to(device)\n",
        "        fake_images = G(z, ages)\n",
        "\n",
        "        D_fake = D(fake_images, ages)\n",
        "        g_loss = real_loss(D_fake)\n",
        "\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        if batch_i % print_every == 0:\n",
        "            losses.append((d_loss.item(), g_loss.item()))\n",
        "            print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
        "                    epoch+1, num_epochs, d_loss.item(), g_loss.item()))\n",
        "\n",
        "    G.eval()\n",
        "    fixed_z = fixed_z.to(device)\n",
        "    fixed_y = fixed_y.to(device)\n",
        "    samples_z = G(fixed_z, fixed_y)\n",
        "    samples.append(samples_z.detach().cpu().numpy())\n",
        "    G.train()\n",
        "\n",
        "    # Save the model and optimizer state dicts\n",
        "    checkpoint(G, D, epoch, model, root_dir)\n",
        "\n",
        "# Save the samples and losses arrays\n",
        "samples_file = os.path.join(root_dir, f'{model}_samples.pkl')\n",
        "losses_file = os.path.join(root_dir, f'{model}_losses.pkl')\n",
        "with open(samples_file, 'wb') as f:\n",
        "    pkl.dump(samples, f)\n",
        "with open(losses_file, 'wb') as f:\n",
        "    pkl.dump(losses, f)\n",
        "\n",
        "# Save the final model and optimizer state dicts\n",
        "checkpoint(G, D, num_epochs, model, root_dir)\n",
        "\n",
        "# Save the final samples array\n",
        "save_samples_ages(samples, fixed_y, model, root_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TiseFz_0kZJ",
        "outputId": "80de8540-0db6-4363-85b4-31e39fbd993b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Discriminator Loss: 0.5835\n",
            "Average Generator Loss: 3.0272\n"
          ]
        }
      ],
      "source": [
        "#Generating losses\n",
        "import numpy as np\n",
        "\n",
        "losses = np.array(losses)\n",
        "d_losses = losses.T[0]\n",
        "g_losses = losses.T[1]\n",
        "avg_d_loss = np.mean(d_losses)\n",
        "avg_g_loss = np.mean(g_losses)\n",
        "print('Average Discriminator Loss: {:.4f}'.format(avg_d_loss))\n",
        "print('Average Generator Loss: {:.4f}'.format(avg_g_loss))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2T2w3CJb0kZK"
      },
      "outputs": [],
      "source": [
        "torch.save(G.state_dict(), os.path.join(root_dir, model+'_G.pth'))\n",
        "torch.save(D.state_dict(), os.path.join(root_dir, model+'_D.pth'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0GFf3Y90kZK",
        "outputId": "699ad5da-2762-4cd2-9f1f-36de27f59f57"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "G.load_state_dict(torch.load(os.path.join(root_dir, model+'_G.pth'), map_location=torch.device(device)))\n",
        "D.load_state_dict(torch.load(os.path.join(root_dir, model+'_D.pth'), map_location=torch.device(device)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqaVaabq0kZL"
      },
      "outputs": [],
      "source": [
        "# Generating sample images\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "\n",
        "num_samples = 16\n",
        "z = torch.randn(num_samples, z_size).to(device)\n",
        "\n",
        "# Ensure all tensors are on the same device\n",
        "ages = torch.zeros(num_samples, len(bins) + 1, device=device).scatter_(\n",
        "    1,\n",
        "    torch.tensor(np.random.randint(len(bins), size=num_samples).reshape(-1, 1), dtype=torch.long, device=device),\n",
        "    1\n",
        ")\n",
        "\n",
        "# Generate fake images\n",
        "fake_images = G(z, ages).detach().cpu()\n",
        "\n",
        "# Create directory for saving samples\n",
        "os.makedirs(os.path.join(root_dir, 'samples'), exist_ok=True)\n",
        "\n",
        "# Save the generated images\n",
        "for i in range(num_samples):\n",
        "    age_label = torch.argmax(ages[i]).item()  # Get the age label as an integer\n",
        "    filename = os.path.join(root_dir, 'samples', f'sample_{i}_age_{age_label}.png')\n",
        "    save_image(fake_images[i], filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J74c2QVK0kZL"
      },
      "outputs": [],
      "source": [
        "for i in range(num_samples):\n",
        "    age_label = torch.argmax(ages[i]).item()  # Extract age label as an integer\n",
        "    filename = os.path.join(root_dir, 'samples', f'sample_{i}_age_{age_label}.png')\n",
        "    save_image(fake_images[i], filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knGnWqnwymr8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAqqI9ELzDxz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipJGPtFd0Jg6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHjTNIi0HHfA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKvQU5AKHxS6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIf7JVto0kZM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}